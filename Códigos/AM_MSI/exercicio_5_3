import numpy as np
from sklearn import preprocessing

from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.decomposition import PCA , SparsePCA, FactorAnalysis
from sklearn.svm import SVR , SVC
from sklearn import neighbors
from sklearn import linear_model
from sklearn.linear_model import ARDRegression, LinearRegression,SGDRegressor
from sklearn.metrics import mean_absolute_error
import collections
from sklearn.cross_decomposition import PLSRegression
import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import MinMaxScaler,Normalizer,MaxAbsScaler
import string
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import StandardScaler

### Nome: Nathana Facion
### Exercicio 5 - Aprendizado de maquina
# OBS: A forma de  realizar de realizar a normalizacao e  imputar os dados cientificamente esta errada
# o correto eh pegar media e desvio de treino e aplicar no teste
# o mesmo eh valido para a imputacao

def meanFinal(acfinal, n_folds):
    return float(acfinal / n_folds)

def accuracy(matrix):
    return float(matrix[0][0] + matrix[1][1]) / float(matrix.sum())

def KNN_C (parameters):
   return GridSearchCV(KNeighborsClassifier(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)


def RN_C (parameters):
   return GridSearchCV(MLPClassifier(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)


def RF_C (parameters):
   return GridSearchCV(RandomForestClassifier(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)


def GBM_C (parameters):
    #parameters = {'learning_rate': [0.1], 'alpha': [0.1], 'n_estimators': [100], 'loss': ['huber'], 'max_depth': [3]
    #    , 'min_samples_split': [272], 'min_samples_leaf': [62], 'subsample': [0.2], 'random_state': [10],
    #              'max_features': [45], 'warm_start': [True], 'min_weight_fraction_leaf': [0.2]}
    return GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)


def  SVR_In (parameters):
    parameters = {'C': [2 ** (0)],'gamma': [2 ** (-10)],'epsilon': [2 ** (-5)],'coef0': [2 ** (0)],'kernel':['rbf'],'degree' : range(1,20,1) }
    #parameters = {'C': [2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)],'gamma':[2 ** (-15), 2 ** (-10), 2 ** (-5), 2 ** (0), 2 ** (5)], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'] ,'degree' : range(1,20,1),
     #             'coef0': [2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)],'epsilon': [2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)], 'tol':[2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)]
     #             }

    # 'C': [2 ** (0)],'gamma': [2 ** (-10)
    #parameters = {'C': [2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)],
    #              'gamma': [2 ** (-15), 2 ** (-10), 2 ** (-5), 2 ** (0), 2 ** (5)]}
    return GridSearchCV(SVR(), parameters,cv=3,  scoring='neg_mean_absolute_error', n_jobs=1)

def KNN (parameters):
    return GridSearchCV(neighbors.KNeighborsRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def RN (parameters):
    return GridSearchCV(MLPRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def RF (parameters):
    return GridSearchCV(RandomForestRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def GBM (parameters):
    return GridSearchCV(GradientBoostingRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def  LM(parameters):
    return GridSearchCV(linear_model.LinearRegression(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def  ARD(parameters):
    return GridSearchCV(ARDRegression(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def  LAR(parameters):
    return GridSearchCV(linear_model.LassoLars(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def LR(parameters):
    return GridSearchCV(linear_model.LogisticRegression(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def SGD(parameters):
    parameters =     {'penalty': ['l2'],'loss': ['epsilon_insensitive'],'alpha':[0.1],'epsilon':[0.5],'average':[2],'power_t': [0.1],'learning_rate':['invscaling'],
                      'l1_ratio':[0.1], 'fit_intercept':[True],'shuffle':[True],'n_iter':[100],'eta0': [0.005],'random_state':[5]}
                #{'loss': ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],
                 # 'penalty': ['l2', 'l1', 'elasticnet']}
                  # ,'alpha':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],'learning_rate':['constant','optimal','invscaling'],'eta0': [0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.001,]
                  # 'epsilon':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],'average':range(1,10,1),'power_t': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],'n_iter': range(90,150,5)}

    return GridSearchCV(linear_model.SGDRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def HG(parameters):
    return GridSearchCV(linear_model.HuberRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def RR(parameters):
    return GridSearchCV(linear_model.Ridge(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)

def PL(parameters):
    return GridSearchCV(PLSRegression(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)


def GBM_TEST():
    parameters = { 'learning_rate' : [0.1], 'alpha' :[0.1], 'n_estimators' : [100], 'loss': ['lad'],  'max_depth': [5]
        ,  'min_samples_split': [30], 'min_samples_leaf' : [62], 'subsample' : [0.2], 'random_state' :[10], 'max_features': ['auto'], 'warm_start' : [True],'min_weight_fraction_leaf':[0.2]}
    print parameters
    return GridSearchCV(GradientBoostingRegressor(), parameters, cv=3, scoring='neg_mean_absolute_error', n_jobs=1)


def kfoldExterno(parameters,X,Y,algorithm):
    n_folds = 2
    external_skf = StratifiedKFold(n_folds)
    acxFinal = 0
    for training_index, test_index in external_skf.split(X,Y):
        X_train, X_test = X[training_index], X[test_index]
        Y_train, Y_test = Y[training_index], Y[test_index]
        if algorithm == 'SVM_C':
            model = SVR_In(parameters)
        elif algorithm == 'KNN_C':
            model = KNN_C(parameters)
        elif algorithm == 'RN_C':  # Redes Neurais
            model = RN_C(parameters)
        elif algorithm == 'RF_C':  # Random Forest
            model = RF_C(parameters)
        elif algorithm == 'GBM_C':  # Gradient Boosting Machine
            model = GBM_C(parameters)
        elif algorithm == 'SVR':
            model = SVR_In(parameters)
        elif algorithm == 'KNN':
            model = KNN(parameters)
        elif algorithm == 'RN':  # Redes Neurais
            model = RN(parameters)
        elif algorithm == 'RF':  # Random Forest
            model = RF(parameters)
        elif algorithm == 'GBM':  # Gradient Boosting Machine
            model = GBM(parameters)
        elif algorithm == 'LM':  # Linear Model
            model = LM(parameters)
        elif algorithm == 'ARD':  # Bayesian ARD regression.
            model = ARD(parameters)
        elif algorithm == 'LAR':  ### Least Angle Regression
            model = LAR(parameters)
        elif algorithm == 'LR':  ### Linear Regression
            model = LR(parameters)
        elif algorithm == 'SGD':  ### Stochastic Gradient Descent
            model = SGD(parameters)
        elif algorithm == 'HG':  ### Huber Regressor
            model = HG(parameters)
        elif algorithm == 'RR':  ### Ridge Regression
            model = RR(parameters)
        elif algorithm == 'PL':
            model = PL(parameters)
        elif algorithm == 'GBM_TEST':
            model = GBM_TEST()
        print('Algorithm:',algorithm)
        model.fit(X_train, Y_train)
        predicted = model.predict(X_test)

        print(model.cv_results_, 'melhor parametro', model.best_params_, model.best_score_)

        ac = mean_absolute_error(Y_test, predicted)
        #print('ac',ac)
        acxFinal = ac + acxFinal

    return meanFinal(acxFinal, n_folds)

def oneHot(X):
    X_new = None
    flag = False
    letter_index = []
    for i in range(X.shape[1]):
        col = X[:, i]
        if i == 0:
            X_new = np.array(pd.get_dummies(X[:, i]))
        else:
            if (not col[0].isdigit()):
                new_col = np.array(pd.get_dummies(X[:, i]))
                X_new = np.append(X_new, new_col, axis=1)
                letter_index.append(i)

    X_deleted = np.delete(X, letter_index, axis=1)
    X_new = np.append(X_new, X_deleted, axis=1)
    return X_new.astype(np.float64)


def buildDataSet():
    fileName = "//home//nathana//train.csv"
    data = open(fileName) # Mude essa linha para fazer a leitura dos dados.
    alphabet = (list(string.lowercase) + list(string.uppercase))
    df = pd.read_csv(data, sep=',', header=None)
    df.columns = [letter for letter in alphabet][0:33]
    df_new = df[alphabet[1:33]]  # remove first column
    df_dic = df_new.T.to_dict().values()
    vec = DictVectorizer()
    X_train = vec.fit_transform(df_dic).toarray()
    #print df[alphabet[0]]
    X_train = preprocessing.scale(X_train)
    Y_train = np.array(df[alphabet[0]])
    count_elements = collections.Counter(Y_train)
    print  'COL)',count_elements, 'size:', len(count_elements)
    return X_train, Y_train

# Realiza pre processamento
def preProcessingData():
    deleteColumns = []
    X , Y = buildDataSet()

    ## Removendo colunas com NaN
    for i in range(X.shape[1]):  # numero de colunas
        col = X[:, i]
     #   #print col
     #   totalNaN = np.count_nonzero(np.isnan(col))
     #   percNaN = (float(totalNaN) / float(col.size))
        count_elements =collections.Counter(col)
        print  i,')',count_elements, 'size:', len(count_elements)
     #   deleteColumns.append(i) if (percNaN > 0.0) else None

    #X = np.delete(X, deleteColumns, axis=1)
    X = np.array(X)
    Y = np.array(Y)
    #normalizer =  Normalizer()
    #normalizer.fit(X)
    #X = normalizer.transform(X)

    scaler = StandardScaler()
    scaler.fit(X)
    X = scaler.transform(X)

    #maxAbsolute = MaxAbsScaler()
    #maxAbsolute.fit(X)
    #X = maxAbsolute.transform(X)

    print 'pca..'
    pca = PCA(n_components=0.99)
    #pca = SparsePCA(n_components = 88)
    #pca = FactorAnalysis(n_components = 150)
    #pca.fit(X)
    X = pca.fit_transform(X)
    print(X.shape[1])
    print(X.shape[0])
    print('preprocessamento...')
    return X, Y


def main():
    X, Y = preProcessingData()


    count_elements = collections.Counter(Y)
    print  'COL)', count_elements, 'size:', len(count_elements)


    cont2 = 0
    for i in count_elements:
        cont = 0
        if count_elements[i] < 200:
            for j in Y:
                #print j
                if i == Y[cont]:
                    Y[cont] = 1
                    cont2 = cont2 + 1
                cont = cont + 1

    print 'cont2:',cont2
    count_elements = collections.Counter(Y)
    print  'COL)', count_elements, 'size:', len(count_elements)

    mean_gbm = kfoldExterno(None, X, Y, 'GBM_TEST')
    print('GBM_TEST:', mean_gbm)

    ### KNN Classificator
    parameters = {'n_neighbors': [1, 5, 11, 15, 21, 25]}
    #mean_knn = kfoldExterno(parameters, X, Y, 'KNN_C')
    #print('KNN:', mean_knn)

    ### Gradient Boosting Machine Classificator
    # Para o GBM (ou XGB) teste para numero de arvores = 30, 70, e 100, com learning rate de 0.1 e 0.05
    parameters = {'learning_rate': [0.1, 0.05], 'max_depth': [5], 'n_estimators': [30, 70, 100]}
    #mean_gbm = kfoldExterno(parameters, X, Y, 'GBM_C')
    #print('GBM C:', mean_gbm)

    ### KNN
    parameters = {'n_neighbors': [1, 5, 11, 15, 21, 25],
                  'metric': ['manhattan', 'chebyshev']}
    #mean_knn = kfoldExterno(parameters, X, Y, 'KNN')
    #print('KNN:', mean_knn)

    ### Linear Model - Tirar esse
    parameters = {}
    #mean_lm = kfoldExterno(parameters, X, Y, 'LM')
    #print('LM:', mean_lm)

    ### Bayesian ARD regression
    parameters = {'alpha_1': [2 ** (-5)],
                  'lambda_1': [2 ** (-5), 2 ** (-3)],
                  }
    #mean_ard = kfoldExterno(parameters, X, Y, 'ARD')
    #print('ARD:', mean_ard)

    ### Least Angle Regression
    parameters = {'eps': [0.1, 0.2, 0.3, 0.4, 0.5]}
    #mean_lar = kfoldExterno(parameters, X, Y, 'LAR')
    #print('LAR:', mean_lar)

    ### Logistic Regression
    parameters = {'C': [2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)]}
    #mean_lr = kfoldExterno(parameters, X, Y, 'LR')
    #print('LR:', mean_lr)

    ### Stochastic Gradient Descent
    parameters = {'loss': ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],
                  'penalty': ['l2', 'l1', 'elasticnet']
                  }
    mean_sgd = kfoldExterno(None, X, Y, 'SGD')
    print('SGD:', mean_sgd)

    ### Huber Regressor
    parameters = {'alpha': [2 ** (-3), 4 ** (-3), 10 ** (-4)],
    }
    mean_hg = kfoldExterno(parameters, X, Y, 'HG')
    print('HG:', mean_hg)

    ###  Ridge Regression
    parameters = {'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']}
    #mean_rr = kfoldExterno(parameters, X, Y, 'RR')
    #print('RR:', mean_rr)

    ### PLSRegression
    parameters = {	'n_components' : [1,2,5,10,20,30,40,50]}
    #mean_pl = kfoldExterno(parameters, X, Y, 'PL')
    #print('PL:', mean_pl)  #### SVR

    parameters = {'C': [2 ** (-5), 2 ** (0), 2 ** (5)],
                  'gamma': [2 ** (-5), 2 ** (0), 2 ** (5)],
                  'kernel': ['rbf', 'linear']}
    # mean_svr = kfoldExterno(parameters, X, Y, 'SVR')
    # print('SVR:',mean_svr)

    #### Rede neural
    parameters = {'hidden_layer_sizes': [10, 20, 40, 80],
                  'solver': ['lbfgs', 'sgd'],
                  'max_iter': [900],
                  'epsilon': [2 ** (-5), 2 ** (0), 2 ** (5)]}
    # mean_rn = kfoldExterno(parameters, X, Y, 'RN')
    # print('RN:',mean_rn)

    #### Random Forest
    parameters = {'max_features': [10, 15, 20, 25],
                  'n_estimators': [100, 200, 300, 400],
                  'max_features': ['sqrt', 'auto', 'log2']}
    # mean_rf = kfoldExterno(parameters, X, Y, 'RF')
    # print('RF:',mean_rf)

    ### Gradient Boosting Machine
    parameters = {'learning_rate': [0.1, 0.05],
                  'loss': ['ls', 'lad', 'huber', 'quantile'],
                  'max_depth': [5],
                  'n_estimators': [30, 70, 100],
                  'max_features': ['sqrt', 'auto', 'log2']
                  }
    # mean_gbm = kfoldExterno(parameters, X, Y, 'GBM')
    # print('GBM:', mean_gbm)

    #### SVM Classificator
    parameters = {'C': [2 ** (-5), 2 ** (0), 2 ** (5), 2 ** (10)],
                  'gamma': [2 ** (-15), 2 ** (-10), 2 ** (-5), 2 ** (0), 2 ** (5)]}
    mean_svm = kfoldExterno(None, X, Y, 'SVM_C')
    print('SVM:', mean_svm)

    #### Rede neural Classificator
    #parameters = {'hidden_layer_sizes': [10, 20, 30, 40], 'max_iter': [400]}
    #mean_rn = kfoldExterno(parameters, X, Y, 'RN_C')
    #print('RN:', mean_rn)

    #### Random Forest Classificator
    #parameters = {'max_features': [10, 15, 20, 25], 'n_estimators': [100, 200, 300, 400]}
    #mean_rf = kfoldExterno(parameters, X, Y, 'RF_C')
    #print('RF C:', mean_rf)

if __name__ == '__main__':
    main()